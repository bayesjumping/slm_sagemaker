{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b1ea92",
   "metadata": {},
   "source": [
    "# Deploy TinyLlama-1.1B-Chat as SageMaker Serverless Endpoint\n",
    "\n",
    "This notebook demonstrates how to deploy the **TinyLlama-1.1B-Chat-v1.0** model as a SageMaker Serverless Inference Endpoint using the HuggingFace Text Generation Inference (TGI) container.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model**: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "- **Endpoint Type**: Serverless (scales to zero when idle)\n",
    "- **Memory**: 3072 MB (3 GB)\n",
    "- **Max Concurrency**: 10\n",
    "- **Cost**: Pay-per-use, ~$0.00002 per second of inference\n",
    "\n",
    "## Benefits of Serverless Endpoints\n",
    "\n",
    "- âœ… No charge when idle (scales to zero)\n",
    "- âœ… Automatic scaling based on traffic\n",
    "- âœ… No instance management\n",
    "- âœ… Ideal for development and intermittent workloads\n",
    "- âš ï¸ Cold start latency (10-60 seconds when inactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2faaef",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "786c328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker SDK version: 3.4.0\n",
      "Boto3 version: 1.42.44\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sagemaker\n",
    "\n",
    "# Get versions safely\n",
    "try:\n",
    "    sm_version = sagemaker.__version__\n",
    "except AttributeError:\n",
    "    import importlib.metadata\n",
    "    sm_version = importlib.metadata.version('sagemaker')\n",
    "\n",
    "print(f\"SageMaker SDK version: {sm_version}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79255a31",
   "metadata": {},
   "source": [
    "## 2. Set Up AWS SageMaker Session\n",
    "\n",
    "Initialize the SageMaker session and get the execution role. The role must have permissions to create SageMaker models and endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a3614",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sagemaker' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize SageMaker session and boto3 clients\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sess = \u001b[43msagemaker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSession\u001b[49m()\n\u001b[32m      3\u001b[39m region = sess.boto_region_name\n\u001b[32m      4\u001b[39m role = sagemaker.get_execution_role()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'sagemaker' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "# Initialize SageMaker session and boto3 clients\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "# Get execution role - works in SageMaker environments or locally\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # Running outside SageMaker, need to specify role ARN manually\n",
    "    # Option 1: Get from existing SageMaker execution roles\n",
    "    iam = boto3.client('iam')\n",
    "    try:\n",
    "        # Try to find an existing SageMaker execution role\n",
    "        roles = iam.list_roles(PathPrefix='/service-role/')\n",
    "        sagemaker_roles = [r for r in roles['Roles'] if 'SageMaker' in r['RoleName']]\n",
    "        if sagemaker_roles:\n",
    "            role = sagemaker_roles[0]['Arn']\n",
    "            print(f\"Using existing SageMaker role: {sagemaker_roles[0]['RoleName']}\")\n",
    "        else:\n",
    "            raise Exception(\"No SageMaker execution role found. Please create one in IAM console.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding role: {e}\")\n",
    "        print(\"\\nPlease specify your SageMaker execution role ARN manually:\")\n",
    "        print(\"role = 'arn:aws:iam::ACCOUNT_ID:role/service-role/YOUR-SAGEMAKER-ROLE'\")\n",
    "        raise\n",
    "\n",
    "# Create SageMaker client for low-level API calls\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "sm_runtime = boto3.client('sagemaker-runtime', region_name=region)\n",
    "\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "print(f\"AWS Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e338e7",
   "metadata": {},
   "source": [
    "## 3. Define Model Configuration\n",
    "\n",
    "Configure the TinyLlama model and TGI (Text Generation Inference) container settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14597293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "endpoint_name = f\"tinyllama-serverless-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# HuggingFace TGI container configuration\n",
    "hub = {\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'HF_TASK': 'text-generation',\n",
    "    'MAX_INPUT_LENGTH': '2048',\n",
    "    'MAX_TOTAL_TOKENS': '4096',\n",
    "    'MAX_BATCH_PREFILL_TOKENS': '4096',\n",
    "}\n",
    "\n",
    "print(f\"Model ID: {model_id}\")\n",
    "print(f\"Endpoint Name: {endpoint_name}\")\n",
    "print(f\"Container Config: {json.dumps(hub, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab967db8",
   "metadata": {},
   "source": [
    "## 4. Create HuggingFace Model\n",
    "\n",
    "Create a HuggingFaceModel instance that will be deployed to SageMaker. This uses the HuggingFace TGI (Text Generation Inference) container optimized for LLM inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HuggingFace TGI container image URI\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"huggingface\",\n",
    "    region=region,\n",
    "    version=\"2.1.1\",\n",
    "    image_scope=\"inference\",\n",
    "    base_framework_version=\"pytorch2.1\",\n",
    "    py_version=\"py310\",\n",
    "    container_version=\"text-generation-inference2.0.1\",\n",
    "    instance_type=\"ml.g5.xlarge\"  # Used for image URI retrieval only\n",
    ")\n",
    "\n",
    "# Create SageMaker model using boto3 client\n",
    "model_name = f\"tinyllama-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': image_uri,\n",
    "        'Environment': hub\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ“ SageMaker Model created successfully\")\n",
    "print(f\"  Model Name: {model_name}\")\n",
    "print(f\"  Image URI: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8a0c7",
   "metadata": {},
   "source": [
    "## 5. Configure Serverless Endpoint\n",
    "\n",
    "Configure the serverless inference settings:\n",
    "- **Memory Size**: 3072 MB (3 GB) - Sufficient for TinyLlama 1.1B model\n",
    "- **Max Concurrency**: 10 - Maximum concurrent invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128930bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serverless configuration values\n",
    "serverless_memory_mb = 3072  # 3 GB - sufficient for TinyLlama 1.1B\n",
    "serverless_max_concurrency = 10  # Maximum concurrent invocations\n",
    "\n",
    "print(\"âœ“ Serverless configuration set\")\n",
    "print(f\"  Memory: {serverless_memory_mb} MB\")\n",
    "print(f\"  Max Concurrency: {serverless_max_concurrency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83ab04",
   "metadata": {},
   "source": [
    "## 6. Deploy Model to Serverless Endpoint\n",
    "\n",
    "Deploy the model to a SageMaker serverless endpoint. This will:\n",
    "1. Create the SageMaker model\n",
    "2. Create the endpoint configuration with serverless settings\n",
    "3. Create and deploy the endpoint\n",
    "\n",
    "**Note**: Initial deployment takes 3-5 minutes for model download and container initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0872f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint configuration with serverless settings\n",
    "endpoint_config_name = f\"tinyllama-config-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': model_name,\n",
    "            'ServerlessConfig': {\n",
    "                'MemorySizeInMB': serverless_memory_mb,\n",
    "                'MaxConcurrency': serverless_max_concurrency\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Endpoint configuration created: {endpoint_config_name}\")\n",
    "\n",
    "# Deploy the endpoint\n",
    "print(f\"ðŸš€ Deploying model to endpoint: {endpoint_name}\")\n",
    "print(\"â³ This will take 3-5 minutes...\")\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "# Wait for endpoint to be in service\n",
    "print(\"â³ Waiting for endpoint to be in service...\")\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "\n",
    "print(f\"âœ… Endpoint deployed successfully!\")\n",
    "print(f\"   Endpoint name: {endpoint_name}\")\n",
    "print(f\"   Endpoint type: Serverless\")\n",
    "print(f\"   Status: InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8b9ad",
   "metadata": {},
   "source": [
    "## 7. Test the Endpoint\n",
    "\n",
    "Send test prompts to the deployed endpoint and verify the model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple prompt\n",
    "test_prompt = \"What is the capital of France?\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": test_prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“ Prompt: {test_prompt}\")\n",
    "print(\"â³ Generating response...\")\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read())\n",
    "\n",
    "print(\"\\nâœ… Response:\")\n",
    "print(\"=\" * 60)\n",
    "print(result[0][\"generated_text\"])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a coding question\n",
    "coding_prompt = \"Write a Python function to calculate the factorial of a number.\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": coding_prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"temperature\": 0.3,  # Lower temperature for more focused code generation\n",
    "        \"top_p\": 0.95,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“ Prompt: {coding_prompt}\")\n",
    "print(\"â³ Generating response...\")\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read())\n",
    "\n",
    "print(\"\\nâœ… Response:\")\n",
    "print(\"=\" * 60)\n",
    "print(result[0][\"generated_text\"])\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f2d51",
   "metadata": {},
   "source": [
    "## 8. Clean Up Resources\n",
    "\n",
    "**Important**: Delete the endpoint when you're done testing to avoid charges.\n",
    "\n",
    "Serverless endpoints scale to zero when idle, so they don't incur charges when not processing requests. However, it's still good practice to delete the endpoint when completely finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint\n",
    "print(f\"ðŸ—‘ï¸  Deleting endpoint: {endpoint_name}\")\n",
    "\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(\"âœ“ Endpoint deleted\")\n",
    "\n",
    "# Optionally delete endpoint configuration\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "print(\"âœ“ Endpoint configuration deleted\")\n",
    "\n",
    "# Optionally delete model\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "print(\"âœ“ Model deleted\")\n",
    "\n",
    "print(\"\\nâœ… All resources cleaned up successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ac8d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Using the Endpoint from Other Applications\n",
    "\n",
    "You can invoke this endpoint from any AWS SDK or CLI:\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName='your-endpoint-name',\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps({\n",
    "        \"inputs\": \"Your prompt here\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    })\n",
    ")\n",
    "\n",
    "result = json.loads(response['Body'].read())\n",
    "print(result[0]['generated_text'])\n",
    "```\n",
    "\n",
    "### Cost Monitoring\n",
    "\n",
    "- Serverless endpoints are billed per inference duration (per millisecond)\n",
    "- No charge when idle (scales to zero)\n",
    "- Typical cost: ~$0.00002 per second of inference with 3GB memory\n",
    "- Monitor costs in AWS Cost Explorer under SageMaker > Inference\n",
    "\n",
    "### Model Optimization\n",
    "\n",
    "For production use, consider:\n",
    "- Fine-tuning the model on your specific use case\n",
    "- Adjusting `max_concurrency` based on expected traffic\n",
    "- Increasing memory if encountering OOM errors\n",
    "- Using quantization for larger models (already applied in this config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
